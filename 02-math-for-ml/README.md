# Vector Similarity Playground

This notebook is part of the "Mathematics for Machine Learning" phase.
It focuses on understanding vectors, cosine similarity, and how words or phrases
can be represented mathematically.

## Objective
- Represent words as vectors
- Compute cosine similarity between words
- Understand why some words/phrases are more similar than others
- Explore combining words to form phrase vectors

## What I Learned
- Basic vector representation of words
- Dot product as a measure of alignment between vectors
- Cosine similarity to compare vectors regardless of magnitude
- Concept of phrase embeddings by summing word vectors
- How to reason mathematically about similarity in AI

## Experiments
- Created a dictionary of words with simple vector representations
- Calculated cosine similarity between different pairs
- Added vectors to simulate phrases
- Observed which words/phrases are more similar

### Example Output
Similarity hello-bye: 0.0
Similarity hello-ok: 0.8164965809277259
Similarity python-code: 0.4999999999999999
Similarity hello-world: 0.7071067811865475
Similarity hello vs 'hello world': 0.9486832980505138